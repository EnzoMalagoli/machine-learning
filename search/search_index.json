{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#entregas-de-machine-learning","title":"Entregas de Machine Learning","text":"<p>Me chamo Enzo Malagoli, e esse pages tem como objetivo ser um template para todas entregas feitas na disciplina de Machine Learning do 4\u00ba semestre de Sistemas de Informa\u00e7\u00e3o na ESPM/SP.</p> <p>Info</p> <ul> <li>Autor: Enzo De Marchi Malagoli</li> <li>Disciplina: Machine Learning</li> <li>Professor: Humberto Sandmann</li> </ul> <p> </p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> \u00c1rvore de Decis\u00e3o - Data 29/08/2025</li> <li> K-Nearest Neighbors - Data 16/09/2025</li> <li> K Means - Data 28/09/2025</li> <li> M\u00e9tricas de Avalia\u00e7\u00e3o - Data 28/09/2025</li> <li> Random Forest - Data 28/10/2025</li> <li> Page Rank - Data 18/11/2025</li> <li> PySpark - Data 18/11/2025</li> <li> Support Vector Machine - Data 04/12/2025</li> <li> Projeto de Dados 1 - Data 30/09/2025</li> <li> Projeto de Dados 2 - Data 05/12/2025</li> </ul>"},{"location":"KMEANS/main/","title":"K-Means","text":"<p>Algoritmo K-Means</p> <p>O algoritmo K-Means \u00e9 uma t\u00e9cnica de aprendizado n\u00e3o supervisionado usada para agrupar dados em k clusters distintos. Ele funciona localizando os centros (centroides) de cada grupo e atribuindo os pontos ao cluster cujo centro esteja mais pr\u00f3ximo, geralmente usando a dist\u00e2ncia euclidiana. O processo \u00e9 iterativo: os pontos s\u00e3o realocados conforme os centroides s\u00e3o recalculados, at\u00e9 que os grupos fiquem est\u00e1veis.</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"KMEANS/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:13:52.362449 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:13:52.564960 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:13:52.664252 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"KMEANS/main/#pre-processamento","title":"Pr\u00e9-processamento","text":"<p>Pr\u00e9-processamento de dados brutos deve ser a primeira etapa ao lidar com datasets de todos tamanhos.</p> <p>Data Cleaning</p> <p>O processo de data cleaning garante que o conjunto utilizado seja confi\u00e1vel e esteja livre de falhas que possam distorcer os resultados. Consiste em identificar e corrigir problemas como valores ausentes, dados inconsistentes ou informa\u00e7\u00f5es que n\u00e3o fazem sentido. Essa limpeza permite que a base seja mais fiel \u00e0 realidade e forne\u00e7a condi\u00e7\u00f5es adequadas para a constru\u00e7\u00e3o de modelos de Machine Learning.</p> <p>No c\u00f3digo, a limpeza foi feita dessa forma: poss\u00edveis valores vazios em idade, g\u00eanero e sal\u00e1rio foram preenchidos com informa\u00e7\u00f5es representativas, como a mediana ou o valor mais frequente.</p> ResultCode Gender Age AnnualSalary Male 32 77500 Female 46 135500 Male 41 73500 Female 47 42500 Female 53 90500 Male 25 59500 Female 41 67500 Male 55 39000 Male 59 135500 Female 36 63000 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    features = ['Gender', 'Age', 'AnnualSalary']\n    return df[features]\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.sample(n=10).to_markdown(index=False))\n</code></pre> <p>Encoding Categorical Variables</p> <p>O processo de encoding de vari\u00e1veis categ\u00f3ricas transforma informa\u00e7\u00f5es em formato de texto em valores num\u00e9ricos, permitindo que algoritmos de Machine Learning consigam utiliz\u00e1-las em seus c\u00e1lculos.</p> <p>No c\u00f3digo, o encoding foi aplicado \u00e0 vari\u00e1vel g\u00eanero, convertendo as categorias \u201cMale\u201d e \u201cFemale\u201d em valores num\u00e9ricos (1 e 0). Dessa forma, a base de dados mant\u00e9m todas as colunas originais, mas agora com a vari\u00e1vel categ\u00f3rica representada de maneira adequada para ser usada em algoritmos de classifica\u00e7\u00e3o.</p> ResultCode User ID Gender Age AnnualSalary Purchased 176 1 41 73500 0 448 1 59 135500 1 391 1 25 59500 0 623 0 47 42500 1 773 0 46 135500 0 413 0 53 90500 1 793 1 55 39000 1 836 0 36 63000 0 586 0 41 67500 0 651 1 32 77500 0 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    # Limpeza\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    # Encoding simples para Gender\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n\n    return df\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.to_markdown(index=False))\n</code></pre> <p>Normaliza\u00e7\u00e3o</p> <p>A normaliza\u00e7\u00e3o \u00e9 o processo de reescalar os valores num\u00e9ricos de forma que fiquem dentro de um intervalo fixo, normalmente entre 0 e 1. Isso facilita a compara\u00e7\u00e3o entre vari\u00e1veis que possuem unidades ou magnitudes diferentes, evitando que atributos com valores muito altos dominem a an\u00e1lise.</p> <p>No c\u00f3digo, a normaliza\u00e7\u00e3o foi aplicada \u00e0s colunas idade e sal\u00e1rio anual, transformando seus valores para a faixa de 0 a 1 por meio do m\u00e9todo Min-Max Scaling. Dessa forma, ambas as vari\u00e1veis passam a estar na mesma escala, tornando o conjunto de dados mais consistente e adequado para a modelagem.</p> ResultCode User ID Gender Age AnnualSalary Purchased 921 Female 0.533333 0.883636 1 484 Male 0.422222 0.810909 1 552 Male 0.933333 0.196364 1 773 Female 0.622222 0.876364 0 457 Male 0.355556 0.203636 0 931 Female 0.555556 0.705455 1 540 Female 0.466667 0.301818 0 385 Male 0.377778 0.0363636 0 39 Male 0.0222222 0.498182 0 876 Male 0.666667 0.214545 0 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# Selecionar colunas num\u00e9ricas para normalizar\nfeatures_to_normalize = ['Age', 'AnnualSalary']\n\n# Inicializar o scaler\nscaler = MinMaxScaler()\n\n# Aplicar normaliza\u00e7\u00e3o e substituir no DataFrame\ndf[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n\n# Mostrar amostra dos dados normalizados\nprint(df.sample(10).to_markdown(index=False))\n</code></pre>"},{"location":"KMEANS/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<p>Ap\u00f3s o pr\u00e9-processamento, o conjunto de dados precisa ser separado em duas partes: uma para treinamento e outra para teste. Essa divis\u00e3o \u00e9 fundamental para que o modelo de Machine Learning aprenda padr\u00f5es a partir de um grupo de exemplos e, depois, seja avaliado em dados que ainda n\u00e3o foram vistos. Dessa forma, \u00e9 poss\u00edvel medir a capacidade de generaliza\u00e7\u00e3o do modelo e evitar que ele apenas memorize os exemplos fornecidos.</p> <p>No c\u00f3digo, os atributos escolhidos como preditores foram g\u00eanero, idade e sal\u00e1rio anual, enquanto a vari\u00e1vel-alvo foi Purchased, que indica se o cliente comprou ou n\u00e3o o produto. A divis\u00e3o foi feita em 70% para treino e 30% para teste, garantindo que a propor\u00e7\u00e3o de clientes que compraram e n\u00e3o compraram fosse preservada em ambos os subconjuntos.</p> ResultCode <p>Tamanho treino: 700 Tamanho teste: 300</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n# --- Separar vari\u00e1veis preditoras \nX = df[[\"Gender\", \"Age\", \"AnnualSalary\"]]\ny = df[\"Purchased\"]\n\n# --- Divis\u00e3o em treino e teste \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Tamanho treino:\", X_train.shape[0])\nprint(\"Tamanho teste:\", X_test.shape[0])\n</code></pre>"},{"location":"KMEANS/main/#implementacao-k-means","title":"Implementa\u00e7\u00e3o K-Means","text":"<p>O processo de clustering foi conduzido utilizando Idade e Sal\u00e1rio Anual como vari\u00e1veis principais, resultando em clusters bem definidos e centr\u00f3ides que representam os perfis m\u00e9dios de cada grupo.</p> <p>Os resultados evidenciaram que a renda anual foi um fator decisivo na forma\u00e7\u00e3o dos clusters, separando clientes em faixas de poder aquisitivo distintas. Essa segmenta\u00e7\u00e3o pode ser \u00fatil para estrat\u00e9gias de marketing, defini\u00e7\u00e3o de p\u00fablico-alvo e personaliza\u00e7\u00e3o de ofertas.</p> ResultCode 2025-12-06T19:13:54.240590 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.cluster import KMeans\n\nplt.figure(figsize=(12, 10))\n\n\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n\nX = df[[\"Age\", \"AnnualSalary\"]].dropna().to_numpy()\n\n\nkmeans = KMeans(n_clusters=2, init=\"k-means++\", max_iter=100, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\"viridis\", s=50, alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c=\"red\", marker=\"*\", s=200, label=\"Centroides\")\n\nplt.title(\"Clusters com K-Means\")\nplt.xlabel(\"Idade\")\nplt.ylabel(\"Sal\u00e1rio Anual\")\nplt.legend()\n\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre> <p>O gr\u00e1fico mostra os clusters formados pelo K-Means, com cada cor representando um grupo de clientes e as estrelas vermelhas indicando os centr\u00f3ides. \u00c9 poss\u00edvel notar que o Sal\u00e1rio Anual foi a vari\u00e1vel mais determinante na separa\u00e7\u00e3o dos grupos, criando um cluster associado a clientes com maior poder aquisitivo e outro relacionado a clientes de renda mais baixa. Assim, o modelo demonstra como o K-Means pode ser usado de forma eficaz para segmenta\u00e7\u00e3o de mercado e an\u00e1lise de perfis de consumidores.</p> <p>Apesar dos bons resultados, \u00e9 importante destacar que o K-Means depende fortemente da escolha do n\u00famero de clusters (K) e pode ser influenciado por outliers.</p>"},{"location":"KNN/main/","title":"K-Nearest Neighbors","text":"<p>Algoritmo KNN</p> <p>O algoritmo K-Nearest Neighbors (KNN) \u00e9 uma t\u00e9cnica de aprendizado supervisionado usada para classifica\u00e7\u00e3o e regress\u00e3o. Ele funciona comparando uma nova amostra com as mais pr\u00f3ximas no conjunto de treino, de acordo com uma medida de dist\u00e2ncia, geralmente a euclidiana. Na etapa de classifica\u00e7\u00e3o, a classe atribu\u00edda \u00e9 definida pela maioria entre os k vizinhos mais pr\u00f3ximos.</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"KNN/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:13:54.522466 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:13:54.692749 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:13:54.792958 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"KNN/main/#pre-processamento","title":"Pr\u00e9-processamento","text":"<p>Pr\u00e9-processamento de dados brutos deve ser a primeira etapa ao lidar com datasets de todos tamanhos.</p> <p>Data Cleaning</p> <p>O processo de data cleaning garante que o conjunto utilizado seja confi\u00e1vel e esteja livre de falhas que possam distorcer os resultados. Consiste em identificar e corrigir problemas como valores ausentes, dados inconsistentes ou informa\u00e7\u00f5es que n\u00e3o fazem sentido. Essa limpeza permite que a base seja mais fiel \u00e0 realidade e forne\u00e7a condi\u00e7\u00f5es adequadas para a constru\u00e7\u00e3o de modelos de Machine Learning.</p> <p>No c\u00f3digo, a limpeza foi feita dessa forma: poss\u00edveis valores vazios em idade, g\u00eanero e sal\u00e1rio foram preenchidos com informa\u00e7\u00f5es representativas, como a mediana ou o valor mais frequente.</p> ResultCode Gender Age AnnualSalary Female 47 42500 Female 41 67500 Male 32 77500 Male 25 59500 Female 53 90500 Female 36 63000 Male 55 39000 Male 41 73500 Female 46 135500 Male 59 135500 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    features = ['Gender', 'Age', 'AnnualSalary']\n    return df[features]\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.sample(n=10).to_markdown(index=False))\n</code></pre> <p>Encoding Categorical Variables</p> <p>O processo de encoding de vari\u00e1veis categ\u00f3ricas transforma informa\u00e7\u00f5es em formato de texto em valores num\u00e9ricos, permitindo que algoritmos de Machine Learning consigam utiliz\u00e1-las em seus c\u00e1lculos.</p> <p>No c\u00f3digo, o encoding foi aplicado \u00e0 vari\u00e1vel g\u00eanero, convertendo as categorias \u201cMale\u201d e \u201cFemale\u201d em valores num\u00e9ricos (1 e 0). Dessa forma, a base de dados mant\u00e9m todas as colunas originais, mas agora com a vari\u00e1vel categ\u00f3rica representada de maneira adequada para ser usada em algoritmos de classifica\u00e7\u00e3o.</p> ResultCode User ID Gender Age AnnualSalary Purchased 176 1 41 73500 0 448 1 59 135500 1 391 1 25 59500 0 623 0 47 42500 1 773 0 46 135500 0 413 0 53 90500 1 793 1 55 39000 1 836 0 36 63000 0 586 0 41 67500 0 651 1 32 77500 0 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    # Limpeza\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    # Encoding simples para Gender\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n\n    return df\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.to_markdown(index=False))\n</code></pre> <p>Normaliza\u00e7\u00e3o</p> <p>A normaliza\u00e7\u00e3o \u00e9 o processo de reescalar os valores num\u00e9ricos de forma que fiquem dentro de um intervalo fixo, normalmente entre 0 e 1. Isso facilita a compara\u00e7\u00e3o entre vari\u00e1veis que possuem unidades ou magnitudes diferentes, evitando que atributos com valores muito altos dominem a an\u00e1lise.</p> <p>No c\u00f3digo, a normaliza\u00e7\u00e3o foi aplicada \u00e0s colunas idade e sal\u00e1rio anual, transformando seus valores para a faixa de 0 a 1 por meio do m\u00e9todo Min-Max Scaling. Dessa forma, ambas as vari\u00e1veis passam a estar na mesma escala, tornando o conjunto de dados mais consistente e adequado para a modelagem.</p> ResultCode User ID Gender Age AnnualSalary Purchased 48 Female 0.311111 0.872727 1 306 Male 0.422222 0.269091 0 650 Male 0.466667 0.407273 0 866 Female 0.6 0.490909 1 86 Male 0.288889 0.534545 0 497 Female 0.6 0.418182 0 164 Male 0.288889 0.0218182 0 771 Male 0.222222 0.541818 0 7 Female 0.733333 0.865455 0 294 Female 0.511111 0.338182 0 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# Selecionar colunas num\u00e9ricas para normalizar\nfeatures_to_normalize = ['Age', 'AnnualSalary']\n\n# Inicializar o scaler\nscaler = MinMaxScaler()\n\n# Aplicar normaliza\u00e7\u00e3o e substituir no DataFrame\ndf[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n\n# Mostrar amostra dos dados normalizados\nprint(df.sample(10).to_markdown(index=False))\n</code></pre>"},{"location":"KNN/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<p>Ap\u00f3s o pr\u00e9-processamento, o conjunto de dados precisa ser separado em duas partes: uma para treinamento e outra para teste. Essa divis\u00e3o \u00e9 fundamental para que o modelo de Machine Learning aprenda padr\u00f5es a partir de um grupo de exemplos e, depois, seja avaliado em dados que ainda n\u00e3o foram vistos. Dessa forma, \u00e9 poss\u00edvel medir a capacidade de generaliza\u00e7\u00e3o do modelo e evitar que ele apenas memorize os exemplos fornecidos.</p> <p>No c\u00f3digo, os atributos escolhidos como preditores foram g\u00eanero, idade e sal\u00e1rio anual, enquanto a vari\u00e1vel-alvo foi Purchased, que indica se o cliente comprou ou n\u00e3o o produto. A divis\u00e3o foi feita em 70% para treino e 30% para teste, garantindo que a propor\u00e7\u00e3o de clientes que compraram e n\u00e3o compraram fosse preservada em ambos os subconjuntos.</p> ResultCode <p>Tamanho treino: 700 Tamanho teste: 300</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n# --- Separar vari\u00e1veis preditoras \nX = df[[\"Gender\", \"Age\", \"AnnualSalary\"]]\ny = df[\"Purchased\"]\n\n# --- Divis\u00e3o em treino e teste \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Tamanho treino:\", X_train.shape[0])\nprint(\"Tamanho teste:\", X_test.shape[0])\n</code></pre>"},{"location":"KNN/main/#implementacao-knn","title":"Implementa\u00e7\u00e3o KNN","text":""},{"location":"KNN/main/#from-scratch","title":"From Scratch","text":"ResultCode <p>Accuracy: 0.91</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nclass KNNClassifier:\n    def __init__(self, k=3):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        return np.array([self._predict(x) for x in X])\n\n    def _predict(self, x):\n        # Dist\u00e2ncia Euclidiana\n        distances = np.sqrt(((self.X_train - x) ** 2).sum(axis=1))\n        # k vizinhos mais pr\u00f3ximos\n        k_idx = np.argsort(distances)[:self.k]\n        k_labels = self.y_train[k_idx]\n        # Classe mais comum\n        vals, counts = np.unique(k_labels, return_counts=True)\n        return vals[np.argmax(counts)]\n\n\ndef preprocess(df):\n    # 1) Data cleaning\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n    df[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n    df[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n    # 2) Encoding (Gender -&gt; 0/1)\n    df[\"Gender\"] = df[\"Gender\"].map({\"Female\": 0, \"Male\": 1})\n\n\n    for col in [\"Age\", \"AnnualSalary\"]:\n        cmin, cmax = df[col].min(), df[col].max()\n        df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\n    X = df[[\"Gender\", \"Age\", \"AnnualSalary\"]].to_numpy(dtype=float)\n    y = df[\"Purchased\"].to_numpy(dtype=int)\n    return X, y\n\n\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\nX, y = preprocess(df)\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42, stratify=y\n)\n\nknn = KNNClassifier(k=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {acc:.2f}\")\n</code></pre>"},{"location":"KNN/main/#usando-scikit-learn","title":"Usando Scikit-Learn","text":"ResultCodeGr\u00e1fico <p>Accuracy: 0.91</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n#  PREPROCESS \ndef preprocess(df):\n\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n    df[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n    df[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n\n    enc = LabelEncoder()\n    df[\"Gender\"] = enc.fit_transform(df[\"Gender\"])\n\n\n    for col in [\"Age\", \"AnnualSalary\"]:\n        cmin, cmax = df[col].min(), df[col].max()\n        df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n    X = df[[\"Gender\", \"Age\", \"AnnualSalary\"]].to_numpy(dtype=float)\n    y = df[\"Purchased\"].to_numpy(dtype=int)\n    return X, y\n\n#  LOAD DATA \nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\nX, y = preprocess(df)\n\n#  SPLIT \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# TRAIN KNN \nknn = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)  # k=5, dist\u00e2ncia Euclidiana\nknn.fit(X_train, y_train)\n\n# PREDICT &amp; EVALUATE\ny_pred = knn.predict(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n</code></pre> <p>Accuracy: 0.90  2025-12-06T19:13:55.121935 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> <p>A etapa de classifica\u00e7\u00e3o foi conduzida utilizando o algoritmo K-Nearest Neighbors (KNN) em duas abordagens distintas. Primeiramente, foi implementado um classificador do zero, no qual as etapas de pr\u00e9-processamento (limpeza, encoding e normaliza\u00e7\u00e3o) foram aplicadas antes da divis\u00e3o em treino e teste. Esse modelo segue a l\u00f3gica central do KNN: calcular dist\u00e2ncias entre os pontos, selecionar os vizinhos mais pr\u00f3ximos e atribuir a classe mais frequente. Apesar de simples, essa vers\u00e3o manual ajudou a compreender os mecanismos internos do algoritmo.</p> <p>Em seguida, a mesma tarefa foi realizada com o KNeighborsClassifier da biblioteca Scikit-Learn, que oferece uma implementa\u00e7\u00e3o mais robusta e otimizada. A acur\u00e1cia obtida em ambas as vers\u00f5es foi semelhante, demonstrando que o pr\u00e9-processamento foi eficaz e que o modelo conseguiu identificar padr\u00f5es relevantes nos dados. Al\u00e9m disso, a visualiza\u00e7\u00e3o da fronteira de decis\u00e3o mostrou de forma clara como o KNN separa os clientes que compraram dos que n\u00e3o compraram, destacando sua capacidade de criar limites n\u00e3o lineares adaptados \u00e0 distribui\u00e7\u00e3o real das vari\u00e1veis de idade e sal\u00e1rio.</p>"},{"location":"arvoredecisao/main/","title":"\u00c1rvore de Decis\u00e3o","text":"<p>Projeto \u00c1rvore de Decis\u00e3o</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"arvoredecisao/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:13:55.453627 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:13:55.620918 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:13:55.717769 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"arvoredecisao/main/#pre-processamento","title":"Pr\u00e9-processamento","text":"<p>Pr\u00e9-processamento de dados brutos deve ser a primeira etapa ao lidar com datasets de todos tamanhos.</p> <p>Data Cleaning</p> <p>O processo de data cleaning garante que o conjunto utilizado seja confi\u00e1vel e esteja livre de falhas que possam distorcer os resultados. Consiste em identificar e corrigir problemas como valores ausentes, dados inconsistentes ou informa\u00e7\u00f5es que n\u00e3o fazem sentido. Essa limpeza permite que a base seja mais fiel \u00e0 realidade e forne\u00e7a condi\u00e7\u00f5es adequadas para a constru\u00e7\u00e3o de modelos de Machine Learning.</p> <p>No c\u00f3digo, a limpeza foi feita dessa forma: poss\u00edveis valores vazios em idade, g\u00eanero e sal\u00e1rio foram preenchidos com informa\u00e7\u00f5es representativas, como a mediana ou o valor mais frequente.</p> ResultCode Gender Age AnnualSalary Female 36 63000 Male 55 39000 Female 53 90500 Male 41 73500 Female 47 42500 Female 41 67500 Male 25 59500 Female 46 135500 Male 59 135500 Male 32 77500 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    features = ['Gender', 'Age', 'AnnualSalary']\n    return df[features]\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.sample(n=10).to_markdown(index=False))\n</code></pre> <p>Encoding Categorical Variables</p> <p>O processo de encoding de vari\u00e1veis categ\u00f3ricas transforma informa\u00e7\u00f5es em formato de texto em valores num\u00e9ricos, permitindo que algoritmos de Machine Learning consigam utiliz\u00e1-las em seus c\u00e1lculos.</p> <p>No c\u00f3digo, o encoding foi aplicado \u00e0 vari\u00e1vel g\u00eanero, convertendo as categorias \u201cMale\u201d e \u201cFemale\u201d em valores num\u00e9ricos (1 e 0). Dessa forma, a base de dados mant\u00e9m todas as colunas originais, mas agora com a vari\u00e1vel categ\u00f3rica representada de maneira adequada para ser usada em algoritmos de classifica\u00e7\u00e3o.</p> ResultCode User ID Gender Age AnnualSalary Purchased 176 1 41 73500 0 448 1 59 135500 1 391 1 25 59500 0 623 0 47 42500 1 773 0 46 135500 0 413 0 53 90500 1 793 1 55 39000 1 836 0 36 63000 0 586 0 41 67500 0 651 1 32 77500 0 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    # Limpeza\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    # Encoding simples para Gender\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n\n    return df\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.to_markdown(index=False))\n</code></pre> <p>Normaliza\u00e7\u00e3o</p> <p>A normaliza\u00e7\u00e3o \u00e9 o processo de reescalar os valores num\u00e9ricos de forma que fiquem dentro de um intervalo fixo, normalmente entre 0 e 1. Isso facilita a compara\u00e7\u00e3o entre vari\u00e1veis que possuem unidades ou magnitudes diferentes, evitando que atributos com valores muito altos dominem a an\u00e1lise.</p> <p>No c\u00f3digo, a normaliza\u00e7\u00e3o foi aplicada \u00e0s colunas idade e sal\u00e1rio anual, transformando seus valores para a faixa de 0 a 1 por meio do m\u00e9todo Min-Max Scaling. Dessa forma, ambas as vari\u00e1veis passam a estar na mesma escala, tornando o conjunto de dados mais consistente e adequado para a modelagem.</p> ResultCode User ID Gender Age AnnualSalary Purchased 493 Female 0.422222 0.305455 0 990 Female 0.688889 0.854545 1 532 Female 0.733333 0.221818 0 960 Male 0.6 0.44 1 551 Female 0.511111 0.272727 0 41 Female 0.4 0.461818 0 828 Male 0.666667 0.0472727 1 229 Male 0.866667 0.327273 1 376 Female 0.422222 0.447273 0 999 Male 0.933333 0.847273 1 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# Selecionar colunas num\u00e9ricas para normalizar\nfeatures_to_normalize = ['Age', 'AnnualSalary']\n\n# Inicializar o scaler\nscaler = MinMaxScaler()\n\n# Aplicar normaliza\u00e7\u00e3o e substituir no DataFrame\ndf[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n\n# Mostrar amostra dos dados normalizados\nprint(df.sample(10).to_markdown(index=False))\n</code></pre>"},{"location":"arvoredecisao/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<p>Ap\u00f3s o pr\u00e9-processamento, o conjunto de dados precisa ser separado em duas partes: uma para treinamento e outra para teste. Essa divis\u00e3o \u00e9 fundamental para que o modelo de Machine Learning aprenda padr\u00f5es a partir de um grupo de exemplos e, depois, seja avaliado em dados que ainda n\u00e3o foram vistos. Dessa forma, \u00e9 poss\u00edvel medir a capacidade de generaliza\u00e7\u00e3o do modelo e evitar que ele apenas memorize os exemplos fornecidos.</p> <p>No c\u00f3digo, os atributos escolhidos como preditores foram g\u00eanero, idade e sal\u00e1rio anual, enquanto a vari\u00e1vel-alvo foi Purchased, que indica se o cliente comprou ou n\u00e3o o produto. A divis\u00e3o foi feita em 70% para treino e 30% para teste, garantindo que a propor\u00e7\u00e3o de clientes que compraram e n\u00e3o compraram fosse preservada em ambos os subconjuntos.</p> ResultCode <p>Tamanho treino: 700 Tamanho teste: 300</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n# --- Separar vari\u00e1veis preditoras \nX = df[[\"Gender\", \"Age\", \"AnnualSalary\"]]\ny = df[\"Purchased\"]\n\n# --- Divis\u00e3o em treino e teste \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Tamanho treino:\", X_train.shape[0])\nprint(\"Tamanho teste:\", X_test.shape[0])\n</code></pre>"},{"location":"arvoredecisao/main/#treinamento-e-avaliacao-do-modelo","title":"Treinamento e Avalia\u00e7\u00e3o do Modelo","text":"<p>Por fim, esta \u00e9 a \u00e1rvore de decis\u00e3o final:</p> Decision TreeDatasetCode <p>Accuracy: 0.9033  2025-12-06T19:13:56.107051 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ </p> Gender Age AnnualSalary Purchased Male 41 73500 0 Male 59 135500 1 Male 25 59500 0 Female 47 42500 1 Female 46 135500 0 Female 53 90500 1 Male 55 39000 1 Female 36 63000 0 Female 41 67500 0 Male 32 77500 0 <pre><code>import matplotlib\nmatplotlib.use(\"Agg\") \nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom io import BytesIO\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\n# PREPROCESS \ndef preprocess(df):\n\n    # Data cleaning\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n    df[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n    df[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n    # Encoding\n    enc = LabelEncoder()\n    df[\"Gender\"] = enc.fit_transform(df[\"Gender\"])  # Female=0, Male=1\n\n    # Normaliza\u00e7\u00e3o Min\u2013Max\n    for col in [\"Age\", \"AnnualSalary\"]:\n        cmin, cmax = df[col].min(), df[col].max()\n        df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n    # Features finais\n    features = [\"Gender\", \"Age\", \"AnnualSalary\"]\n    return df[features]\n\n# CARREGAR DADOS \nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# X (features) / y (target)\nX = preprocess(df)\ny = df[\"Purchased\"]\n\n# TRAIN / TEST SPLIT \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# DECISION TREE\nclf = tree.DecisionTreeClassifier(max_depth=4, random_state=42) \nclf.fit(X_train, y_train)\n\n# AVALIA\u00c7\u00c3O \ny_pred = clf.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {acc:.4f}\")\n\n#  PLOTAR \u00c1RVORE \nplt.figure(figsize=(14, 10))\ntree.plot_tree(\n    clf,\n    feature_names=X.columns.tolist(),\n    class_names=[\"N\u00e3o comprou (0)\", \"Comprou (1)\"],\n    filled=True, rounded=True, fontsize=9\n)\n\nbuf = BytesIO()\nplt.savefig(buf, format=\"svg\", bbox_inches=\"tight\", transparent=True)\nbuf.seek(0)\nprint(buf.getvalue().decode(\"utf-8\"))\nplt.close()\n</code></pre> <p>O modelo de \u00e1rvore de decis\u00e3o foi treinado com as vari\u00e1veis idade, g\u00eanero e sal\u00e1rio anual, atingindo uma acur\u00e1cia de aproximadamente 90%. A an\u00e1lise da \u00e1rvore mostra que idade e sal\u00e1rio foram os principais fatores utilizados para separar compradores e n\u00e3o compradores, enquanto o g\u00eanero teve impacto secund\u00e1rio.</p>"},{"location":"arvoredecisao/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O projeto teve in\u00edcio com a explora\u00e7\u00e3o do dataset, etapa em que foi poss\u00edvel identificar padr\u00f5es relevantes, como uma leve predomin\u00e2ncia do p\u00fablico feminino e a influ\u00eancia direta de idade e renda no comportamento de compra. Durante essa fase, tamb\u00e9m foi necess\u00e1rio lidar com valores ausentes e normalizar vari\u00e1veis para tornar o conjunto adequado \u00e0 modelagem.</p> <p>Ap\u00f3s o pr\u00e9-processamento, o modelo de \u00e1rvore de decis\u00e3o foi treinado e alcan\u00e7ou uma acur\u00e1cia em torno de 90%, indicando boa capacidade de identificar corretamente os clientes com maior ou menor propens\u00e3o \u00e0 compra. A an\u00e1lise da \u00e1rvore revelou que idade e sal\u00e1rio anual s\u00e3o os fatores que mais impactam as previs\u00f5es, enquanto o g\u00eanero aparece em pontos espec\u00edficos, mas com menor import\u00e2ncia.</p> <p>Em linhas gerais, o trabalho atendeu ao objetivo de desenvolver um prot\u00f3tipo preditivo baseado em dados demogr\u00e1ficos e socioecon\u00f4micos. O modelo se mostrou eficiente e, com ajustes adicionais, tem potencial para ser expandido em aplica\u00e7\u00f5es mais completas no apoio a decis\u00f5es de mercado.</p>"},{"location":"metricas/main/","title":"M\u00e9tricas","text":"<p>Metrics and Evaluation - Algoritmo KNN e K-Means</p> <p>Os algoritmos K-Nearest Neighbors (KNN) e K-Means s\u00e3o duas abordagens cl\u00e1ssicas de Machine Learning que, embora diferentes em sua natureza, compartilham a ideia central de medir a proximidade entre pontos. O KNN \u00e9 um m\u00e9todo supervisionado, utilizado para tarefas de classifica\u00e7\u00e3o, em que novas amostras s\u00e3o atribu\u00eddas \u00e0 classe predominante entre seus vizinhos mais pr\u00f3ximos. J\u00e1 o K-Means \u00e9 um m\u00e9todo n\u00e3o supervisionado, aplicado para agrupar dados em clusters com base na semelhan\u00e7a de suas caracter\u00edsticas, sem depender de r\u00f3tulos previamente definidos.</p> <p>Ao serem aplicados em conjunto, esses algoritmos permitem duas perspectivas complementares: de um lado, a predi\u00e7\u00e3o direta de categorias por meio do KNN; de outro, a descoberta de padr\u00f5es ocultos com o K-Means. Essa combina\u00e7\u00e3o ajuda tanto a entender melhor a estrutura dos dados quanto a explorar possibilidades de segmenta\u00e7\u00e3o e an\u00e1lise de comportamento.</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"metricas/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:13:56.594978 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:13:56.765914 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:13:56.864571 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"metricas/main/#pre-processamento","title":"Pr\u00e9-processamento","text":"<p>Pr\u00e9-processamento de dados brutos deve ser a primeira etapa ao lidar com datasets de todos tamanhos.</p> <p>Data Cleaning</p> <p>O processo de data cleaning garante que o conjunto utilizado seja confi\u00e1vel e esteja livre de falhas que possam distorcer os resultados. Consiste em identificar e corrigir problemas como valores ausentes, dados inconsistentes ou informa\u00e7\u00f5es que n\u00e3o fazem sentido. Essa limpeza permite que a base seja mais fiel \u00e0 realidade e forne\u00e7a condi\u00e7\u00f5es adequadas para a constru\u00e7\u00e3o de modelos de Machine Learning.</p> <p>No c\u00f3digo, a limpeza foi feita dessa forma: poss\u00edveis valores vazios em idade, g\u00eanero e sal\u00e1rio foram preenchidos com informa\u00e7\u00f5es representativas, como a mediana ou o valor mais frequente.</p> ResultCode Gender Age AnnualSalary Female 41 67500 Male 25 59500 Male 55 39000 Female 53 90500 Male 41 73500 Female 46 135500 Male 32 77500 Male 59 135500 Female 36 63000 Female 47 42500 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    features = ['Gender', 'Age', 'AnnualSalary']\n    return df[features]\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.sample(n=10).to_markdown(index=False))\n</code></pre> <p>Encoding Categorical Variables</p> <p>O processo de encoding de vari\u00e1veis categ\u00f3ricas transforma informa\u00e7\u00f5es em formato de texto em valores num\u00e9ricos, permitindo que algoritmos de Machine Learning consigam utiliz\u00e1-las em seus c\u00e1lculos.</p> <p>No c\u00f3digo, o encoding foi aplicado \u00e0 vari\u00e1vel g\u00eanero, convertendo as categorias \u201cMale\u201d e \u201cFemale\u201d em valores num\u00e9ricos (1 e 0). Dessa forma, a base de dados mant\u00e9m todas as colunas originais, mas agora com a vari\u00e1vel categ\u00f3rica representada de maneira adequada para ser usada em algoritmos de classifica\u00e7\u00e3o.</p> ResultCode User ID Gender Age AnnualSalary Purchased 176 1 41 73500 0 448 1 59 135500 1 391 1 25 59500 0 623 0 47 42500 1 773 0 46 135500 0 413 0 53 90500 1 793 1 55 39000 1 836 0 36 63000 0 586 0 41 67500 0 651 1 32 77500 0 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    # Limpeza\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    # Encoding simples para Gender\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n\n    return df\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.to_markdown(index=False))\n</code></pre> <p>Normaliza\u00e7\u00e3o</p> <p>A normaliza\u00e7\u00e3o \u00e9 o processo de reescalar os valores num\u00e9ricos de forma que fiquem dentro de um intervalo fixo, normalmente entre 0 e 1. Isso facilita a compara\u00e7\u00e3o entre vari\u00e1veis que possuem unidades ou magnitudes diferentes, evitando que atributos com valores muito altos dominem a an\u00e1lise.</p> <p>No c\u00f3digo, a normaliza\u00e7\u00e3o foi aplicada \u00e0s colunas idade e sal\u00e1rio anual, transformando seus valores para a faixa de 0 a 1 por meio do m\u00e9todo Min-Max Scaling. Dessa forma, ambas as vari\u00e1veis passam a estar na mesma escala, tornando o conjunto de dados mais consistente e adequado para a modelagem.</p> ResultCode User ID Gender Age AnnualSalary Purchased 677 Female 0.666667 0.272727 1 972 Male 0.266667 0.669091 1 418 Female 0.222222 0.210909 0 343 Female 0.888889 0.789091 1 364 Female 0.511111 0.447273 0 930 Male 0.666667 0.189091 1 340 Female 0.777778 0.0545455 1 126 Male 0.177778 0.472727 0 984 Male 0.4 0.330909 0 542 Female 0.644444 0.44 0 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# Selecionar colunas num\u00e9ricas para normalizar\nfeatures_to_normalize = ['Age', 'AnnualSalary']\n\n# Inicializar o scaler\nscaler = MinMaxScaler()\n\n# Aplicar normaliza\u00e7\u00e3o e substituir no DataFrame\ndf[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n\n# Mostrar amostra dos dados normalizados\nprint(df.sample(10).to_markdown(index=False))\n</code></pre>"},{"location":"metricas/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<p>Ap\u00f3s o pr\u00e9-processamento, o conjunto de dados precisa ser separado em duas partes: uma para treinamento e outra para teste. Essa divis\u00e3o \u00e9 fundamental para que o modelo de Machine Learning aprenda padr\u00f5es a partir de um grupo de exemplos e, depois, seja avaliado em dados que ainda n\u00e3o foram vistos. Dessa forma, \u00e9 poss\u00edvel medir a capacidade de generaliza\u00e7\u00e3o do modelo e evitar que ele apenas memorize os exemplos fornecidos.</p> <p>No c\u00f3digo, os atributos escolhidos como preditores foram g\u00eanero, idade e sal\u00e1rio anual, enquanto a vari\u00e1vel-alvo foi Purchased, que indica se o cliente comprou ou n\u00e3o o produto. A divis\u00e3o foi feita em 70% para treino e 30% para teste, garantindo que a propor\u00e7\u00e3o de clientes que compraram e n\u00e3o compraram fosse preservada em ambos os subconjuntos.</p> ResultCode <p>Tamanho treino: 700 Tamanho teste: 300</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n# --- Separar vari\u00e1veis preditoras \nX = df[[\"Gender\", \"Age\", \"AnnualSalary\"]]\ny = df[\"Purchased\"]\n\n# --- Divis\u00e3o em treino e teste \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Tamanho treino:\", X_train.shape[0])\nprint(\"Tamanho teste:\", X_test.shape[0])\n</code></pre>"},{"location":"metricas/main/#implementacao-knn","title":"Implementa\u00e7\u00e3o KNN","text":""},{"location":"metricas/main/#usando-scikit-learn","title":"Usando Scikit-Learn","text":"<p>A aplica\u00e7\u00e3o do KNN resultou em uma boa performance na tarefa de classifica\u00e7\u00e3o, com acur\u00e1cia pr\u00f3xima de 91%. O modelo conseguiu diferenciar de forma consistente as classes 0 (n\u00e3o comprou) e 1 (comprou), mostrando que a proximidade entre idade, g\u00eanero e sal\u00e1rio anual \u00e9 suficiente para prever o comportamento de compra. A matriz de confus\u00e3o evidenciou poucos erros, refor\u00e7ando a capacidade do algoritmo em generalizar padr\u00f5es do conjunto de dados.</p> ResultCode <p>Accuracy: 0.91</p> <p>Confusion Matrix: |        |   Pred 0 |   Pred 1 | |:-------|---------:|---------:| | True 0 |      164 |       15 | | True 1 |       11 |      110 |</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom io import BytesIO\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef preprocess(df):\n\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n    df[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n    df[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n    enc = LabelEncoder()\n    df[\"Gender\"] = enc.fit_transform(df[\"Gender\"]) \n\n    for col in [\"Age\", \"AnnualSalary\"]:\n        cmin, cmax = df[col].min(), df[col].max()\n        df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n    # X / y\n    X = df[[\"Gender\", \"Age\", \"AnnualSalary\"]].to_numpy(float)\n    y = df[\"Purchased\"].to_numpy(int)\n    return X, y\n\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\")\nX, y = preprocess(df)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42, stratify=y\n)\n\n\nknn = KNeighborsClassifier(n_neighbors=5) \nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"]).to_markdown())\n</code></pre>"},{"location":"metricas/main/#implementacao-k-means","title":"Implementa\u00e7\u00e3o K-Means","text":"<p>A an\u00e1lise com o algoritmo K-Means permitiu identificar dois grupos principais no conjunto de dados, definidos a partir da combina\u00e7\u00e3o entre idade e sal\u00e1rio anual. Cada cor no gr\u00e1fico representa um cluster, enquanto as estrelas vermelhas marcam os centr\u00f3ides, ou seja, os pontos m\u00e9dios que caracterizam cada grupo. Essa separa\u00e7\u00e3o evidencia padr\u00f5es de comportamento entre os indiv\u00edduos, como faixas salariais e idades que tendem a se agrupar. Apesar de n\u00e3o utilizar os r\u00f3tulos originais (como no KNN), o K-Means oferece uma vis\u00e3o explorat\u00f3ria \u00fatil para identificar tend\u00eancias e estruturas ocultas nos dados.</p> Gr\u00e1ficoCode 2025-12-06T19:13:57.037836 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO\nfrom sklearn.cluster import KMeans\n\nplt.figure(figsize=(12, 10))\n\n\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n\nX = df[[\"Age\", \"AnnualSalary\"]].dropna().to_numpy()\n\n\nkmeans = KMeans(n_clusters=2, init=\"k-means++\", max_iter=100, random_state=42, n_init=10)\nlabels = kmeans.fit_predict(X)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\"viridis\", s=50, alpha=0.7)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            c=\"red\", marker=\"*\", s=200, label=\"Centroides\")\n\nplt.title(\"Clusters com K-Means\")\nplt.xlabel(\"Idade\")\nplt.ylabel(\"Sal\u00e1rio Anual\")\nplt.legend()\n\n\nbuffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</code></pre> <p>As m\u00e9tricas de avalia\u00e7\u00e3o desempenham um papel essencial na valida\u00e7\u00e3o de modelos de machine learning, pois permitem medir sua performance de forma objetiva. No caso do KNN, foram utilizadas m\u00e9tricas supervisionadas como acur\u00e1cia, matriz de confus\u00e3o, precis\u00e3o, recall e F1-score, que mostraram um desempenho consistente, com acur\u00e1cia em torno de 91% e boa distin\u00e7\u00e3o entre as classes 0 (n\u00e3o comprou) e 1 (comprou). </p> <p>J\u00e1 no K-Means, por ser um algoritmo n\u00e3o supervisionado, a an\u00e1lise foi realizada a partir da coer\u00eancia entre os clusters formados e a vari\u00e1vel real de compra, o que evidenciou padr\u00f5es relevantes no comportamento dos dados. Assim, observa-se que a escolha de m\u00e9tricas adequadas, supervisionadas para classifica\u00e7\u00e3o e comparativas para clustering, \u00e9 indispens\u00e1vel para interpretar corretamente os resultados obtidos.</p>"},{"location":"pagerank/main/","title":"Page Rank","text":"<p>PageRank Algorithm</p> <p>O PageRank \u00e9 um algoritmo baseado em grafos desenvolvido por Larry Page e Sergey Brin para medir a import\u00e2ncia relativa de n\u00f3s em uma rede. Ele modela a navega\u00e7\u00e3o como um \u201csurfista aleat\u00f3rio\u201d que, a cada passo, segue um link de sa\u00edda da p\u00e1gina atual com certa probabilidade e, com a probabilidade complementar, teleporta para qualquer outro n\u00f3 do grafo. Dessa forma, p\u00e1ginas (ou v\u00e9rtices) que recebem muitos links de outros n\u00f3s importantes acumulam um valor de PageRank mais elevado, refletindo maior relev\u00e2ncia estrutural dentro da rede.</p> <p>Matematicamente, o PageRank \u00e9 definido como a distribui\u00e7\u00e3o estacion\u00e1ria de uma cadeia de Markov sobre o grafo, controlada por um fator de amortecimento que equilibra a influ\u00eancia da estrutura de links com o comportamento aleat\u00f3rio do usu\u00e1rio. Essa abordagem reduz o impacto de ru\u00eddos locais, \u00e9 relativamente robusta a manipula\u00e7\u00f5es simples de links e permite ranquear n\u00f3s em redes grandes de forma eficiente, tornando-se um dos algoritmos fundamentais em an\u00e1lise de redes complexas.</p> <p>High Energy Physics Citation Network</p> <p>Neste projeto, o PageRank \u00e9 aplicado a uma rede de cita\u00e7\u00f5es cient\u00edficas, em que cada n\u00f3 representa um artigo e cada aresta dirigida A \u2192 B indica que o artigo A cita o artigo B. Diferentemente de um problema cl\u00e1ssico de classifica\u00e7\u00e3o supervisionada, o objetivo aqui \u00e9 medir a import\u00e2ncia estrutural dos trabalhos dentro do campo, identificando quais papers funcionam como refer\u00eancias centrais porque s\u00e3o citados por muitos outros artigos relevantes.</p> <p>A partir do dataset de F\u00edsica de Altas Energias, a rede de cita\u00e7\u00f5es \u00e9 carregada como um grafo dirigido e o algoritmo PageRank \u00e9 implementado do zero, seguindo o modelo do surfista aleat\u00f3rio. Em seguida, os resultados s\u00e3o comparados com a implementa\u00e7\u00e3o pronta do NetworkX, analisando-se os dez artigos com maior score e discutindo como a varia\u00e7\u00e3o do fator de amortecimento afeta o ranqueamento. Dessa forma, o exerc\u00edcio conecta teoria de grafos, m\u00e9todos num\u00e9ricos e interpreta\u00e7\u00e3o substantiva dos n\u00f3s mais influentes na literatura cient\u00edfica.</p> <p>Constru\u00e7\u00e3o do Grafo</p> <p>O primeiro passo consiste em carregar o arquivo de arestas Cit-HepTh.txt, em que cada linha representa uma cita\u00e7\u00e3o entre dois artigos. Esse arquivo \u00e9 lido com o NetworkX e transformado em um grafo dirigido, preservando o sentido das arestas: uma aresta de u para v indica que o paper u cita o paper v.</p> Code <pre><code>import networkx as nx\n\n\ndef load_directed_graph(edge_list_path):\n    G = nx.read_edgelist(\n        edge_list_path,\n        create_using=nx.DiGraph(),\n        nodetype=int,\n        comments=\"#\"\n    )\n    return G\n\n\ndef load_undirected_as_bidirected(edge_list_path):\n    G_und = nx.read_edgelist(\n        edge_list_path,\n        create_using=nx.Graph(),\n        nodetype=int,\n        comments=\"#\"\n    )\n    G = nx.DiGraph()\n    for u, v in G_und.edges():\n        G.add_edge(u, v)\n        G.add_edge(v, u)\n    return G\n</code></pre> <p>Info</p> <p>Essa etapa garante que a estrutura de cita\u00e7\u00f5es do dataset seja representada corretamente como um grafo dirigido, permitindo aplicar o PageRank diretamente sobre a rede cient\u00edfica.</p> <p>Implementa\u00e7\u00e3o do PageRank From Scratch</p> <p>A implementa\u00e7\u00e3o do PageRank segue a formula\u00e7\u00e3o iterativa do modelo do surfista aleat\u00f3rio. Inicialmente, todos os n\u00f3s recebem o mesmo valor de PageRank, e a cada itera\u00e7\u00e3o o score de cada v\u00e9rtice \u00e9 atualizado a partir das contribui\u00e7\u00f5es dos n\u00f3s que apontam para ele, normalizadas pelo n\u00famero de sa\u00eddas. N\u00f3s sem arestas de sa\u00edda (dangling nodes) t\u00eam sua massa redistribu\u00edda uniformemente por todo o grafo. O processo continua at\u00e9 que a diferen\u00e7a entre duas itera\u00e7\u00f5es consecutivas seja menor que uma toler\u00e2ncia pr\u00e9-definida.</p> <p>A fun\u00e7\u00e3o abaixo recebe um grafo dirigido do NetworkX e devolve um dicion\u00e1rio com os valores de PageRank para cada n\u00f3, permitindo ajustar o fator de amortecimento, a toler\u00e2ncia de converg\u00eancia e o n\u00famero m\u00e1ximo de itera\u00e7\u00f5es.</p> Code <pre><code>import networkx as nx\n\n\ndef pagerank_custom(G, d=0.85, tol=1.0e-6, max_iter=100):\n    if not G.is_directed():\n        G = G.to_directed()\n\n    nodes = list(G.nodes())\n    N = len(nodes)\n\n    if N == 0:\n        return {}\n\n    pr = {n: 1.0 / N for n in nodes}\n\n    for _ in range(max_iter):\n        new_pr = {}\n        dangling_mass = sum(pr[n] for n in nodes if G.out_degree(n) == 0)\n\n        for i in nodes:\n            rank = (1.0 - d) / N\n            rank += d * dangling_mass / N\n\n            for j in G.predecessors(i):\n                out_deg_j = G.out_degree(j)\n                if out_deg_j &gt; 0:\n                    rank += d * pr[j] / out_deg_j\n\n            new_pr[i] = rank\n\n        diff = sum(abs(new_pr[n] - pr[n]) for n in nodes)\n        pr = new_pr\n\n        if diff &lt; tol:\n            break\n\n    return pr\n</code></pre> <p>Info</p> <p>Essa implementa\u00e7\u00e3o reproduz o comportamento do PageRank cl\u00e1ssico, incluindo o tratamento de n\u00f3s sem sa\u00edda e o uso do damping factor como par\u00e2metro, o que permite posteriormente analisar o impacto de diferentes valores de d na distribui\u00e7\u00e3o final de import\u00e2ncia.</p> <p>Execu\u00e7\u00e3o dos Experimentos</p> <p>Para organizar os experimentos foi criado um script principal respons\u00e1vel por:</p> <p>Carregar o dataset Cit-HepTh.txt como grafo dirigido.</p> <p>Aplicar o pagerank_custom para diferentes valores de damping factor (por exemplo, 0.5, 0.85 e 0.99).</p> <p>Calcular o PageRank usando a fun\u00e7\u00e3o networkx.pagerank com os mesmos par\u00e2metros.</p> <p>Comparar os resultados das duas abordagens, medindo a diferen\u00e7a m\u00e1xima entre os vetores de PageRank.</p> <p>Identificar os dez n\u00f3s com maior PageRank em cada configura\u00e7\u00e3o, permitindo analisar quais artigos se destacam como mais influentes na rede de cita\u00e7\u00f5es.</p> Code <pre><code>import networkx as nx\n\n\ndef load_directed_graph(edge_list_path):\n    G = nx.read_edgelist(\n        edge_list_path,\n        create_using=nx.DiGraph(),\n        nodetype=int,\n        comments=\"#\",\n    )\n    return G\n\n\ndef load_undirected_as_bidirected(edge_list_path):\n    G_und = nx.read_edgelist(\n        edge_list_path,\n        create_using=nx.Graph(),\n        nodetype=int,\n        comments=\"#\",\n    )\n    G = nx.DiGraph()\n    for u, v in G_und.edges():\n        G.add_edge(u, v)\n        G.add_edge(v, u)\n    return G\n\n\ndef pagerank_custom(G, d=0.85, tol=1.0e-6, max_iter=100):\n    if not G.is_directed():\n        G = G.to_directed()\n\n    nodes = list(G.nodes())\n    N = len(nodes)\n\n    if N == 0:\n        return {}\n\n    pr = {n: 1.0 / N for n in nodes}\n\n    for _ in range(max_iter):\n        new_pr = {}\n        dangling_mass = sum(pr[n] for n in nodes if G.out_degree(n) == 0)\n\n        for i in nodes:\n            rank = (1.0 - d) / N\n            rank += d * dangling_mass / N\n\n            for j in G.predecessors(i):\n                out_deg_j = G.out_degree(j)\n                if out_deg_j &gt; 0:\n                    rank += d * pr[j] / out_deg_j\n\n            new_pr[i] = rank\n\n        diff = sum(abs(new_pr[n] - pr[n]) for n in nodes)\n        pr = new_pr\n\n        if diff &lt; tol:\n            break\n\n    return pr\n\n\ndef top_k(pr_dict, k=10):\n    return sorted(pr_dict.items(), key=lambda x: x[1], reverse=True)[:k]\n\n\ndef run_experiment(edge_list_path, directed=True, ds=(0.5, 0.85, 0.99), tol=1e-6, max_iter=100):\n    print(\"Iniciando experimento de PageRank...\")\n\n    if directed:\n        G = load_directed_graph(edge_list_path)\n    else:\n        G = load_undirected_as_bidirected(edge_list_path)\n\n    print(f\"Grafo carregado: {G.number_of_nodes()} n\u00f3s, {G.number_of_edges()} arestas.\\n\")\n\n    for d in ds:\n        print(f\"=== PageRank com damping factor d = {d} ===\")\n\n        pr_custom = pagerank_custom(G, d=d, tol=tol, max_iter=max_iter)\n        pr_nx = nx.pagerank(G, alpha=d, tol=tol)\n\n        all_nodes = pr_custom.keys()\n        max_diff = max(abs(pr_custom[n] - pr_nx[n]) for n in all_nodes)\n        print(f\"M\u00e1xima diferen\u00e7a entre custom e networkx: {max_diff:.2e}\")\n\n        top10 = top_k(pr_custom, k=10)\n        print(\"Top 10 n\u00f3s (n\u00f3, PageRank):\")\n        for node, score in top10:\n            print(f\"{node}\\t{score:.6f}\")\n        print()\n\n\nif __name__ == \"__main__\":\n    EDGE_LIST_PATH = \"data/Cit-HepTh.txt\"\n    run_experiment(EDGE_LIST_PATH, directed=True)\n</code></pre> <p>Info</p> <p>Esse script centraliza a l\u00f3gica experimental: leitura do grafo, chamada da implementa\u00e7\u00e3o pr\u00f3pria, compara\u00e7\u00e3o com a biblioteca e exibi\u00e7\u00e3o dos n\u00f3s mais importantes para cada valor de d.</p> <p>Resultados e An\u00e1lise</p> <p>Ao executar o experimento, observa-se que a implementa\u00e7\u00e3o pr\u00f3pria do PageRank converge em poucas itera\u00e7\u00f5es e produz valores extremamente pr\u00f3ximos aos obtidos pela fun\u00e7\u00e3o networkx.pagerank, com diferen\u00e7as num\u00e9ricas muito pequenas. Isso indica que o algoritmo implementado segue corretamente a formula\u00e7\u00e3o do PageRank e serve como uma valida\u00e7\u00e3o pr\u00e1tica do m\u00e9todo desenvolvido do zero.</p> <p>A an\u00e1lise dos dez n\u00f3s com maior PageRank revela os artigos mais centrais na rede de F\u00edsica de Altas Energias. Esses papers tendem a ser citados por muitos outros trabalhos que tamb\u00e9m ocupam posi\u00e7\u00f5es de destaque, formando um n\u00facleo de refer\u00eancias fundamentais na \u00e1rea. Em termos de interpreta\u00e7\u00e3o, s\u00e3o artigos que funcionam como \u201chubs\u201d de cita\u00e7\u00e3o: resultados te\u00f3ricos importantes, revis\u00f5es amplamente utilizadas ou contribui\u00e7\u00f5es que servem de base para diversos estudos subsequentes.</p> <p>A varia\u00e7\u00e3o do damping factor permite observar como o comportamento do surfista aleat\u00f3rio afeta o ranqueamento. Com valores mais baixos, como d = 0.5, a distribui\u00e7\u00e3o de PageRank se torna mais uniforme, pois o teleporte ocorre com maior frequ\u00eancia e reduz a influ\u00eancia de caminhos longos de cita\u00e7\u00e3o. J\u00e1 com d pr\u00f3ximo de 1, como 0.99, o algoritmo privilegia ainda mais a estrutura do grafo, concentrando a import\u00e2ncia em componentes fortemente conectadas e aumentando a diferen\u00e7a entre os artigos mais citados e o restante da rede. O valor intermedi\u00e1rio d = 0.85, cl\u00e1ssico na literatura, oferece um equil\u00edbrio entre explora\u00e7\u00e3o aleat\u00f3ria e respeito \u00e0 estrutura de links, produzindo rankings est\u00e1veis e interpret\u00e1veis.</p> <p>De forma geral, o exerc\u00edcio mostra como o PageRank pode ser utilizado al\u00e9m da web, servindo como ferramenta para identificar n\u00f3s centrais em diferentes tipos de redes. No contexto de cita\u00e7\u00f5es cient\u00edficas, ele destaca papers que desempenham papel estruturante na literatura, auxiliando na descoberta de trabalhos seminais e na compreens\u00e3o da organiza\u00e7\u00e3o do conhecimento em uma \u00e1rea espec\u00edfica.</p>"},{"location":"pyspark/main/","title":"PySpark","text":"<p>Random Forest Learning Algorithm</p> <p>Os Random Forests s\u00e3o um m\u00e9todo de ensemble que combina v\u00e1rias \u00e1rvores de decis\u00e3o para aumentar a precis\u00e3o e a capacidade de generaliza\u00e7\u00e3o do modelo. A ideia central \u00e9 introduzir aleatoriedade controlada: cada \u00e1rvore \u00e9 treinada sobre um bootstrap (amostra com reposi\u00e7\u00e3o) do conjunto de dados e, a cada divis\u00e3o, considera apenas um subconjunto aleat\u00f3rio de atributos. Essa estrat\u00e9gia (bagging + sele\u00e7\u00e3o aleat\u00f3ria de features) descorrela as \u00e1rvores individuais, reduz a vari\u00e2ncia e torna o modelo mais robusto a ru\u00eddo e overfitting. Em classifica\u00e7\u00e3o, a predi\u00e7\u00e3o final \u00e9 dada pelo voto da maioria; em regress\u00e3o, pela m\u00e9dia das previs\u00f5es.</p> <p>O Random Forest oferece import\u00e2ncias de atributos e a estimativa de erro out-of-bag (OOB), que funciona como uma valida\u00e7\u00e3o embutida sem precisar de holdout extra. Entre os pontos de aten\u00e7\u00e3o est\u00e3o o custo computacional e a menor interpretabilidade quando comparado a uma \u00fanica \u00e1rvore. No geral, \u00e9 uma escolha s\u00f3lida e vers\u00e1til para dados tabulares, equilibrando baixo vi\u00e9s das \u00e1rvores profundas com baixa vari\u00e2ncia obtida pelo ensemble.</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"pyspark/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:13:57.340399 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:13:57.509332 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:13:57.603570 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"pyspark/main/#pre-processamento","title":"Pr\u00e9-processamento","text":"<p>Pr\u00e9-processamento de dados brutos deve ser a primeira etapa ao lidar com datasets de todos tamanhos.</p> <p>Data Cleaning</p> <p>O processo de data cleaning garante que o conjunto utilizado seja confi\u00e1vel e esteja livre de falhas que possam distorcer os resultados. Consiste em identificar e corrigir problemas como valores ausentes, dados inconsistentes ou informa\u00e7\u00f5es que n\u00e3o fazem sentido. Essa limpeza permite que a base seja mais fiel \u00e0 realidade e forne\u00e7a condi\u00e7\u00f5es adequadas para a constru\u00e7\u00e3o de modelos de Machine Learning.</p> <p>No c\u00f3digo, a limpeza foi feita dessa forma: poss\u00edveis valores vazios em idade, g\u00eanero e sal\u00e1rio foram preenchidos com informa\u00e7\u00f5es representativas, como a mediana ou o valor mais frequente.</p> ResultCode Gender Age AnnualSalary Female 41 67500 Male 41 73500 Male 59 135500 Male 32 77500 Male 25 59500 Female 53 90500 Male 55 39000 Female 47 42500 Female 36 63000 Female 46 135500 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    features = ['Gender', 'Age', 'AnnualSalary']\n    return df[features]\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.sample(n=10).to_markdown(index=False))\n</code></pre> <p>Encoding Categorical Variables</p> <p>O processo de encoding de vari\u00e1veis categ\u00f3ricas transforma informa\u00e7\u00f5es em formato de texto em valores num\u00e9ricos, permitindo que algoritmos de Machine Learning consigam utiliz\u00e1-las em seus c\u00e1lculos.</p> <p>No c\u00f3digo, o encoding foi aplicado \u00e0 vari\u00e1vel g\u00eanero, convertendo as categorias \u201cMale\u201d e \u201cFemale\u201d em valores num\u00e9ricos (1 e 0). Dessa forma, a base de dados mant\u00e9m todas as colunas originais, mas agora com a vari\u00e1vel categ\u00f3rica representada de maneira adequada para ser usada em algoritmos de classifica\u00e7\u00e3o.</p> ResultCode User ID Gender Age AnnualSalary Purchased 176 1 41 73500 0 448 1 59 135500 1 391 1 25 59500 0 623 0 47 42500 1 773 0 46 135500 0 413 0 53 90500 1 793 1 55 39000 1 836 0 36 63000 0 586 0 41 67500 0 651 1 32 77500 0 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    # Limpeza\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    # Encoding simples para Gender\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n\n    return df\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.to_markdown(index=False))\n</code></pre> <p>Normaliza\u00e7\u00e3o</p> <p>A normaliza\u00e7\u00e3o \u00e9 o processo de reescalar os valores num\u00e9ricos de forma que fiquem dentro de um intervalo fixo, normalmente entre 0 e 1. Isso facilita a compara\u00e7\u00e3o entre vari\u00e1veis que possuem unidades ou magnitudes diferentes, evitando que atributos com valores muito altos dominem a an\u00e1lise.</p> <p>No c\u00f3digo, a normaliza\u00e7\u00e3o foi aplicada \u00e0s colunas idade e sal\u00e1rio anual, transformando seus valores para a faixa de 0 a 1 por meio do m\u00e9todo Min-Max Scaling. Dessa forma, ambas as vari\u00e1veis passam a estar na mesma escala, tornando o conjunto de dados mais consistente e adequado para a modelagem.</p> ResultCode User ID Gender Age AnnualSalary Purchased 649 Female 0.222222 0.523636 0 396 Female 0.177778 0 0 959 Male 0.2 0.545455 0 2 Female 0.444444 0.352727 0 867 Female 0.533333 0.425455 0 920 Female 0.533333 0.12 0 243 Female 0.288889 0.538182 0 6 Female 0.4 0.221818 0 713 Female 0.0222222 0.0436364 0 358 Male 0.466667 0.629091 1 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# Selecionar colunas num\u00e9ricas para normalizar\nfeatures_to_normalize = ['Age', 'AnnualSalary']\n\n# Inicializar o scaler\nscaler = MinMaxScaler()\n\n# Aplicar normaliza\u00e7\u00e3o e substituir no DataFrame\ndf[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n\n# Mostrar amostra dos dados normalizados\nprint(df.sample(10).to_markdown(index=False))\n</code></pre>"},{"location":"pyspark/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<p>Ap\u00f3s o pr\u00e9-processamento, o conjunto de dados precisa ser separado em duas partes: uma para treinamento e outra para teste. Essa divis\u00e3o \u00e9 fundamental para que o modelo de Machine Learning aprenda padr\u00f5es a partir de um grupo de exemplos e, depois, seja avaliado em dados que ainda n\u00e3o foram vistos. Dessa forma, \u00e9 poss\u00edvel medir a capacidade de generaliza\u00e7\u00e3o do modelo e evitar que ele apenas memorize os exemplos fornecidos.</p> <p>No c\u00f3digo, os atributos escolhidos como preditores foram g\u00eanero, idade e sal\u00e1rio anual, enquanto a vari\u00e1vel-alvo foi Purchased, que indica se o cliente comprou ou n\u00e3o o produto. A divis\u00e3o foi feita em 70% para treino e 30% para teste, garantindo que a propor\u00e7\u00e3o de clientes que compraram e n\u00e3o compraram fosse preservada em ambos os subconjuntos.</p> ResultCode <p>Tamanho treino: 700 Tamanho teste: 300</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n# --- Separar vari\u00e1veis preditoras \nX = df[[\"Gender\", \"Age\", \"AnnualSalary\"]]\ny = df[\"Purchased\"]\n\n# --- Divis\u00e3o em treino e teste \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Tamanho treino:\", X_train.shape[0])\nprint(\"Tamanho teste:\", X_test.shape[0])\n</code></pre>"},{"location":"randomforest/main/","title":"Random Forest","text":"<p>Random Forest Learning Algorithm</p> <p>Os Random Forests s\u00e3o um m\u00e9todo de ensemble que combina v\u00e1rias \u00e1rvores de decis\u00e3o para aumentar a precis\u00e3o e a capacidade de generaliza\u00e7\u00e3o do modelo. A ideia central \u00e9 introduzir aleatoriedade controlada: cada \u00e1rvore \u00e9 treinada sobre um bootstrap (amostra com reposi\u00e7\u00e3o) do conjunto de dados e, a cada divis\u00e3o, considera apenas um subconjunto aleat\u00f3rio de atributos. Essa estrat\u00e9gia (bagging + sele\u00e7\u00e3o aleat\u00f3ria de features) descorrela as \u00e1rvores individuais, reduz a vari\u00e2ncia e torna o modelo mais robusto a ru\u00eddo e overfitting. Em classifica\u00e7\u00e3o, a predi\u00e7\u00e3o final \u00e9 dada pelo voto da maioria; em regress\u00e3o, pela m\u00e9dia das previs\u00f5es.</p> <p>O Random Forest oferece import\u00e2ncias de atributos e a estimativa de erro out-of-bag (OOB), que funciona como uma valida\u00e7\u00e3o embutida sem precisar de holdout extra. Entre os pontos de aten\u00e7\u00e3o est\u00e3o o custo computacional e a menor interpretabilidade quando comparado a uma \u00fanica \u00e1rvore. No geral, \u00e9 uma escolha s\u00f3lida e vers\u00e1til para dados tabulares, equilibrando baixo vi\u00e9s das \u00e1rvores profundas com baixa vari\u00e2ncia obtida pelo ensemble.</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"randomforest/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:13:57.848724 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:13:58.018420 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:13:58.120904 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"randomforest/main/#pre-processamento","title":"Pr\u00e9-processamento","text":"<p>Pr\u00e9-processamento de dados brutos deve ser a primeira etapa ao lidar com datasets de todos tamanhos.</p> <p>Data Cleaning</p> <p>O processo de data cleaning garante que o conjunto utilizado seja confi\u00e1vel e esteja livre de falhas que possam distorcer os resultados. Consiste em identificar e corrigir problemas como valores ausentes, dados inconsistentes ou informa\u00e7\u00f5es que n\u00e3o fazem sentido. Essa limpeza permite que a base seja mais fiel \u00e0 realidade e forne\u00e7a condi\u00e7\u00f5es adequadas para a constru\u00e7\u00e3o de modelos de Machine Learning.</p> <p>No c\u00f3digo, a limpeza foi feita dessa forma: poss\u00edveis valores vazios em idade, g\u00eanero e sal\u00e1rio foram preenchidos com informa\u00e7\u00f5es representativas, como a mediana ou o valor mais frequente.</p> ResultCode Gender Age AnnualSalary Male 59 135500 Female 41 67500 Female 53 90500 Male 41 73500 Male 32 77500 Female 47 42500 Female 46 135500 Male 25 59500 Male 55 39000 Female 36 63000 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    features = ['Gender', 'Age', 'AnnualSalary']\n    return df[features]\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.sample(n=10).to_markdown(index=False))\n</code></pre> <p>Encoding Categorical Variables</p> <p>O processo de encoding de vari\u00e1veis categ\u00f3ricas transforma informa\u00e7\u00f5es em formato de texto em valores num\u00e9ricos, permitindo que algoritmos de Machine Learning consigam utiliz\u00e1-las em seus c\u00e1lculos.</p> <p>No c\u00f3digo, o encoding foi aplicado \u00e0 vari\u00e1vel g\u00eanero, convertendo as categorias \u201cMale\u201d e \u201cFemale\u201d em valores num\u00e9ricos (1 e 0). Dessa forma, a base de dados mant\u00e9m todas as colunas originais, mas agora com a vari\u00e1vel categ\u00f3rica representada de maneira adequada para ser usada em algoritmos de classifica\u00e7\u00e3o.</p> ResultCode User ID Gender Age AnnualSalary Purchased 176 1 41 73500 0 448 1 59 135500 1 391 1 25 59500 0 623 0 47 42500 1 773 0 46 135500 0 413 0 53 90500 1 793 1 55 39000 1 836 0 36 63000 0 586 0 41 67500 0 651 1 32 77500 0 <pre><code>import pandas as pd\n\ndef preprocess(df):\n    # Limpeza\n    df['Age'].fillna(df['Age'].median(), inplace=True)\n    df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n    df['AnnualSalary'].fillna(df['AnnualSalary'].median(), inplace=True)\n\n    # Encoding simples para Gender\n    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n\n\n    return df\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv')\ndf = df.sample(n=10, random_state=42)\ndf = preprocess(df)\n\n\nprint(df.to_markdown(index=False))\n</code></pre> <p>Normaliza\u00e7\u00e3o</p> <p>A normaliza\u00e7\u00e3o \u00e9 o processo de reescalar os valores num\u00e9ricos de forma que fiquem dentro de um intervalo fixo, normalmente entre 0 e 1. Isso facilita a compara\u00e7\u00e3o entre vari\u00e1veis que possuem unidades ou magnitudes diferentes, evitando que atributos com valores muito altos dominem a an\u00e1lise.</p> <p>No c\u00f3digo, a normaliza\u00e7\u00e3o foi aplicada \u00e0s colunas idade e sal\u00e1rio anual, transformando seus valores para a faixa de 0 a 1 por meio do m\u00e9todo Min-Max Scaling. Dessa forma, ambas as vari\u00e1veis passam a estar na mesma escala, tornando o conjunto de dados mais consistente e adequado para a modelagem.</p> ResultCode User ID Gender Age AnnualSalary Purchased 195 Female 0.755556 0.876364 0 141 Female 0.866667 0.658182 1 863 Male 0.444444 0.32 0 885 Male 0.711111 0.156364 1 651 Male 0.311111 0.454545 0 247 Female 0.555556 0.941818 1 190 Female 0.377778 0.407273 0 769 Male 0.444444 0.334545 0 448 Male 0.911111 0.876364 1 551 Female 0.511111 0.272727 0 <pre><code>import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# Selecionar colunas num\u00e9ricas para normalizar\nfeatures_to_normalize = ['Age', 'AnnualSalary']\n\n# Inicializar o scaler\nscaler = MinMaxScaler()\n\n# Aplicar normaliza\u00e7\u00e3o e substituir no DataFrame\ndf[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n\n# Mostrar amostra dos dados normalizados\nprint(df.sample(10).to_markdown(index=False))\n</code></pre>"},{"location":"randomforest/main/#divisao-dos-dados","title":"Divis\u00e3o dos Dados","text":"<p>Ap\u00f3s o pr\u00e9-processamento, o conjunto de dados precisa ser separado em duas partes: uma para treinamento e outra para teste. Essa divis\u00e3o \u00e9 fundamental para que o modelo de Machine Learning aprenda padr\u00f5es a partir de um grupo de exemplos e, depois, seja avaliado em dados que ainda n\u00e3o foram vistos. Dessa forma, \u00e9 poss\u00edvel medir a capacidade de generaliza\u00e7\u00e3o do modelo e evitar que ele apenas memorize os exemplos fornecidos.</p> <p>No c\u00f3digo, os atributos escolhidos como preditores foram g\u00eanero, idade e sal\u00e1rio anual, enquanto a vari\u00e1vel-alvo foi Purchased, que indica se o cliente comprou ou n\u00e3o o produto. A divis\u00e3o foi feita em 70% para treino e 30% para teste, garantindo que a propor\u00e7\u00e3o de clientes que compraram e n\u00e3o compraram fosse preservada em ambos os subconjuntos.</p> ResultCode <p>Tamanho treino: 700 Tamanho teste: 300</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n# --- Separar vari\u00e1veis preditoras \nX = df[[\"Gender\", \"Age\", \"AnnualSalary\"]]\ny = df[\"Purchased\"]\n\n# --- Divis\u00e3o em treino e teste \nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\nprint(\"Tamanho treino:\", X_train.shape[0])\nprint(\"Tamanho teste:\", X_test.shape[0])\n</code></pre>"},{"location":"randomforest/main/#implementacao-random-forest","title":"Implementa\u00e7\u00e3o Random Forest","text":""},{"location":"randomforest/main/#from-scratch","title":"From Scratch","text":"ResultCode <p>Accuracy: 0.91  Confusion Matrix: |        |   Pred 0 |   Pred 1 | |:-------|---------:|---------:| | True 0 |      167 |       12 | | True 1 |       15 |      105 | </p> <pre><code>import random\nimport pandas as pd\nfrom collections import Counter\n\n# ===== PREPROCESS =====\ndef preprocess(df):\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n    df[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n    df[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n    df[\"Gender\"] = df[\"Gender\"].map({\"Female\": 0, \"Male\": 1})\n\n    for col in [\"Age\", \"AnnualSalary\"]:\n        cmin, cmax = df[col].min(), df[col].max()\n        df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n    X = df[[\"Gender\", \"Age\", \"AnnualSalary\"]].values.tolist()\n    y = df[\"Purchased\"].astype(int).tolist()\n    return X, y\n\n# ===== LOAD DATA =====\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\nX, y = preprocess(df)\n\n# ===== STRATIFIED SPLIT (listas, sem numpy) =====\ndef stratified_train_test_split(X, y, test_size=0.3, seed=42):\n    rnd = random.Random(seed)\n    idx0 = [i for i, t in enumerate(y) if t == 0]\n    idx1 = [i for i, t in enumerate(y) if t == 1]\n    rnd.shuffle(idx0); rnd.shuffle(idx1)\n    n0_test = max(1, int(len(idx0) * test_size))\n    n1_test = max(1, int(len(idx1) * test_size))\n    test_idx = idx0[:n0_test] + idx1[:n1_test]\n    train_idx = idx0[n0_test:] + idx1[n1_test:]\n    def take(ix): \n        return [X[i] for i in ix], [y[i] for i in ix]\n    return *take(train_idx), *take(test_idx)\n\nX_train, y_train, X_test, y_test = stratified_train_test_split(X, y, test_size=0.3, seed=42)\n\n# ===== GINI, TREE, FOREST (seu estilo) =====\ndef gini_impurity(y):\n    if not y:\n        return 0.0\n    counts = Counter(y)\n    imp = 1.0\n    n = len(y)\n    for c in counts.values():\n        p = c / n\n        imp -= p * p\n    return imp\n\ndef split_dataset(X, y, feature_idx, value):\n    left_X, left_y, right_X, right_y = [], [], [], []\n    for i in range(len(X)):\n        if X[i][feature_idx] &lt;= value:\n            left_X.append(X[i]); left_y.append(y[i])\n        else:\n            right_X.append(X[i]); right_y.append(y[i])\n    return left_X, left_y, right_X, right_y\n\nclass Node:\n    def __init__(self, feature_idx=None, value=None, left=None, right=None, label=None):\n        self.feature_idx = feature_idx\n        self.value = value\n        self.left = left\n        self.right = right\n        self.label = label\n\ndef build_tree(X, y, max_depth, min_samples_split, max_features, rnd):\n    if len(y) &lt; min_samples_split or max_depth == 0:\n        return Node(label=Counter(y).most_common(1)[0][0])\n\n    n_features = len(X[0])\n    features = rnd.sample(range(n_features), max_features)\n\n    best_gini = float(\"inf\")\n    best = None\n\n    for f in features:\n        values = sorted({row[f] for row in X})\n        for v in values:\n            LX, Ly, RX, Ry = split_dataset(X, y, f, v)\n            if not Ly or not Ry:\n                continue\n            pL = len(Ly) / len(y)\n            g = pL * gini_impurity(Ly) + (1 - pL) * gini_impurity(Ry)\n            if g &lt; best_gini:\n                best_gini = g\n                best = (f, v, LX, Ly, RX, Ry)\n\n    if best is None:\n        return Node(label=Counter(y).most_common(1)[0][0])\n\n    f, v, LX, Ly, RX, Ry = best\n    left = build_tree(LX, Ly, max_depth - 1, min_samples_split, max_features, rnd)\n    right = build_tree(RX, Ry, max_depth - 1, min_samples_split, max_features, rnd)\n    return Node(f, v, left, right)\n\ndef predict_tree(node, x):\n    if node.label is not None:\n        return node.label\n    if x[node.feature_idx] &lt;= node.value:\n        return predict_tree(node.left, x)\n    else:\n        return predict_tree(node.right, x)\n\nclass RandomForest:\n    def __init__(self, n_estimators=25, max_depth=6, min_samples_split=2, max_features=\"sqrt\", seed=42):\n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n        self.seed = seed\n        self.trees = []\n\n    def fit(self, X, y):\n        rnd = random.Random(self.seed)\n        n = len(y)\n        n_features = len(X[0])\n        mf = int(n_features ** 0.5) if self.max_features == \"sqrt\" else self.max_features\n\n        for _ in range(self.n_estimators):\n            idx = [rnd.randint(0, n - 1) for _ in range(n)]\n            Xb = [X[i] for i in idx]\n            yb = [y[i] for i in idx]\n            tree = build_tree(Xb, yb, self.max_depth, self.min_samples_split, mf, rnd)\n            self.trees.append(tree)\n\n    def predict(self, X):\n        preds = []\n        for x in X:\n            votes = [predict_tree(t, x) for t in self.trees]\n            preds.append(Counter(votes).most_common(1)[0][0])\n        return preds\n\nrf = RandomForest(n_estimators=50, max_depth=6, min_samples_split=2, max_features=\"sqrt\", seed=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nacc = sum(1 for yp, yt in zip(y_pred, y_test) if yp == yt) / len(y_test)\nprint(f\"Accuracy: {acc:.2f}\")\n\ncm = [[0, 0], [0, 0]]\nfor yt, yp in zip(y_test, y_pred):\n    cm[yt][yp] += 1\ncm_df = pd.DataFrame(cm, index=[\"True 0\", \"True 1\"], columns=[\"Pred 0\", \"Pred 1\"])\nprint(\"\\nConfusion Matrix:\")\nprint(cm_df.to_markdown())\n</code></pre> <p>O modelo Random Forest implementado manualmente apresentou uma acur\u00e1cia de aproximadamente 90,33%, com uma boa capacidade de generaliza\u00e7\u00e3o mesmo sem t\u00e9cnicas de otimiza\u00e7\u00e3o internas como o Out-of-Bag (OOB). A matriz de confus\u00e3o indica 161 verdadeiros negativos e 110 verdadeiros positivos, com poucos erros de classifica\u00e7\u00e3o: 18 falsos positivos e 11 falsos negativos. Esses resultados demonstram que a estrutura de vota\u00e7\u00e3o entre m\u00faltiplas \u00e1rvores gerou um modelo robusto, capaz de distinguir corretamente os padr\u00f5es de compra e n\u00e3o compra na maioria dos casos. Ainda que mais simples que a vers\u00e3o de biblioteca, a implementa\u00e7\u00e3o manual reproduz de forma fiel o comportamento esperado do algoritmo, confirmando a efici\u00eancia do m\u00e9todo mesmo em sua forma b\u00e1sica.    </p>"},{"location":"randomforest/main/#com-scikit-learn","title":"Com Scikit-Learn","text":"ResultCode <p>Accuracy: 0.90 OOB Score: 0.89  Confusion Matrix: |        |   Pred 0 |   Pred 1 | |:-------|---------:|---------:| | True 0 |      161 |       18 | | True 1 |       12 |      109 |  Feature Importances: | Feature      |   Importance | |:-------------|-------------:| | AnnualSalary |   0.506804   | | Age          |   0.486348   | | Gender       |   0.00684796 | </p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\n\n# ===== PREPROCESS =====\ndef preprocess(df):\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n    df[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n    df[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n    enc = LabelEncoder()\n    df[\"Gender\"] = enc.fit_transform(df[\"Gender\"])  # Female=0, Male=1\n\n    for col in [\"Age\", \"AnnualSalary\"]:\n        cmin, cmax = df[col].min(), df[col].max()\n        df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n    X = df[[\"Gender\", \"Age\", \"AnnualSalary\"]].to_numpy(float)\n    y = df[\"Purchased\"].to_numpy(int)\n    return X, y, [\"Gender\", \"Age\", \"AnnualSalary\"]\n\n# ===== DATA =====\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\nX, y, feat_names = preprocess(df)\n\n# ===== SPLIT =====\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42, stratify=y\n)\n\n# ===== MODEL =====\nrf = RandomForestClassifier(\n    n_estimators=300,\n    max_depth=None,\n    max_features=\"sqrt\",\n    bootstrap=True,\n    oob_score=True,\n    n_jobs=-1,\n    random_state=42\n)\nrf.fit(X_train, y_train)\n\n# ===== EVAL =====\ny_pred = rf.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\nprint(f\"Accuracy: {acc:.2f}\")\nprint(f\"OOB Score: {getattr(rf, 'oob_score_', float('nan')):.2f}\")\n\ncm_df = pd.DataFrame(cm, index=[\"True 0\", \"True 1\"], columns=[\"Pred 0\", \"Pred 1\"])\nprint(\"\\nConfusion Matrix:\")\nprint(cm_df.to_markdown())\n\nimp = pd.DataFrame({\"Feature\": feat_names, \"Importance\": rf.feature_importances_}).sort_values(\"Importance\", ascending=False)\nprint(\"\\nFeature Importances:\")\nprint(imp.to_markdown(index=False))\n</code></pre> <p>A vers\u00e3o utilizando a biblioteca Scikit-Learn obteve desempenho praticamente id\u00eantico, com 90% de acur\u00e1cia e OOB score de 89%, o que refor\u00e7a a estabilidade do modelo e sua boa capacidade de generaliza\u00e7\u00e3o. A an\u00e1lise das import\u00e2ncias das vari\u00e1veis mostra que os atributos AnnualSalary (\u22480,51) e Age (\u22480,49) s\u00e3o os principais determinantes da decis\u00e3o de compra, enquanto Gender (\u22480,007) tem influ\u00eancia m\u00ednima. Essa distribui\u00e7\u00e3o refor\u00e7a que o comportamento do consumidor no dataset est\u00e1 fortemente associado \u00e0 renda e \u00e0 idade, e n\u00e3o a fatores demogr\u00e1ficos secund\u00e1rios. Assim, o modelo se mostra bem calibrado, coerente e interpret\u00e1vel, com desempenho consistente em ambas as abordagens.</p>"},{"location":"roteiro2/main/","title":"Main","text":""},{"location":"roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"roteiro3/main/","title":"Main","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> </p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> <p></p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"roteiro4/main/","title":"Main","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-12-06T19:14:00.130366 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ 2025-12-06T19:14:00.921744 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"svm/main/","title":"SVM","text":"<p>Support Vector Machine</p> <p>Support Vector Machine (SVM) \u00e9 um algoritmo supervisionado usado principalmente para classifica\u00e7\u00e3o que busca encontrar o hiperplano que melhor separa as classes, maximizando a margem entre os pontos de fronteira (os support vectors) e essa linha/hiperplano. Intuitivamente, ele n\u00e3o tenta apenas \u201cacertar os r\u00f3tulos\u201d no treino, mas encontrar uma separa\u00e7\u00e3o o mais ampla e est\u00e1vel poss\u00edvel, o que tende a gerar melhor generaliza\u00e7\u00e3o em dados novos. Em cen\u00e1rios com sobreposi\u00e7\u00e3o ou ru\u00eddo, o SVM usa o conceito de soft margin, controlado pelo par\u00e2metro C, permitindo alguns erros em troca de uma fronteira mais robusta.</p> <p>Quando os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis no espa\u00e7o original, entra o kernel trick: em vez de desenhar uma reta em 2D, o SVM projeta implicitamente os dados em um espa\u00e7o de dimens\u00e3o maior, onde a separa\u00e7\u00e3o passa a ser linear. O kernel RBF, por exemplo, cria fronteiras de decis\u00e3o curvas que se adaptam a padr\u00f5es mais complexos. No contexto do problema de compra de carros, o SVM aprende uma fronteira de decis\u00e3o n\u00e3o linear em fun\u00e7\u00e3o de g\u00eanero, idade e sal\u00e1rio anual para distinguir entre clientes que tendem a comprar ou n\u00e3o o ve\u00edculo, usando apenas alguns pontos-chave (support vectors) para definir essa fronteira.</p> <p>Cars Purchase Decision</p> <p>Este projeto tem como objetivo aplicar t\u00e9cnicas de Machine Learning para compreender os fatores que influenciam a decis\u00e3o de compra de autom\u00f3veis. A partir de um conjunto de dados com informa\u00e7\u00f5es sobre idade, g\u00eanero e sal\u00e1rio anual dos clientes, foi constru\u00edda uma \u00e1rvore de decis\u00e3o capaz de classificar se um indiv\u00edduo provavelmente realizar\u00e1 a compra ou n\u00e3o.</p>"},{"location":"svm/main/#exploracao-dos-dados","title":"Explora\u00e7\u00e3o dos Dados","text":"<p>Estat\u00edsticas Descritivas</p> <p>Para o projeto foi utilizado o dataset Cars - Purchase Decision Dataset e cont\u00e9m detalhes de clientes que consideraram comprar um autom\u00f3vel, juntamente com seus sal\u00e1rios.</p> <p>O conjunto de dados cont\u00e9m 1000 registros e 5 vari\u00e1veis. A vari\u00e1vel alvo \u00e9 Purchased (0 = n\u00e3o comprou, 1 = comprou). Entre as vari\u00e1veis explicativas, temos Gender (categ\u00f3rica), Age (num\u00e9rica) e AnnualSalary (num\u00e9rica).</p> <p>Vari\u00e1veis</p> <ul> <li> <p>User ID: C\u00f3digo do Cliente</p> </li> <li> <p>Gender: G\u00eanero do Cliente</p> </li> <li> <p>Age: Idade do Cliente em anos</p> </li> <li> <p>AnnualSalary: Sal\u00e1rio anual do Cliente</p> </li> <li> <p>Purchased: Se o cliente realizou a compra</p> </li> </ul> <p>Estat\u00edsticas Descritivas e Visualiza\u00e7\u00f5es</p> <p>O gr\u00e1fico mostra a rela\u00e7\u00e3o entre idade e sal\u00e1rio dos clientes, destacando quem realizou a compra e quem n\u00e3o comprou:</p> ResultCode 2025-12-06T19:14:01.233499 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- ETAPA 2: Encoding\ndf[\"Gender\"] = df[\"Gender\"].map({\"Male\": 1, \"Female\": 0})\n\n# --- ETAPA 3: Normaliza\u00e7\u00e3o\nfor col in [\"Age\", \"AnnualSalary\"]:\n    cmin, cmax = df[col].min(), df[col].max()\n    df[col] = 0.0 if cmax == cmin else (df[col] - cmin) / (cmax - cmin)\n\n\ndf0 = df[df[\"Purchased\"] == 0]\ndf1 = df[df[\"Purchased\"] == 1]\n\n# --- PLOT: Dispers\u00e3o Idade x Sal\u00e1rio ---\nfig, ax = plt.subplots(1, 1, figsize=(7, 5))\n\nax.scatter(\n    df0[\"Age\"], df0[\"AnnualSalary\"],\n    label=\"N\u00e3o comprou (0)\", alpha=0.4,\n    color=\"lightcoral\", edgecolor=\"darkred\", linewidth=0.8\n)\nax.scatter(\n    df1[\"Age\"], df1[\"AnnualSalary\"],\n    label=\"Comprou (1)\", alpha=0.4,\n    color=\"skyblue\", edgecolor=\"navy\", linewidth=0.8\n)\n\nax.set_title(\"Idade x Sal\u00e1rio por Decis\u00e3o de Compra\")\nax.set_xlabel(\"Idade\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.grid(linestyle=\"--\", alpha=0.6)\nax.legend()\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>A visualiza\u00e7\u00e3o deixa claro que idade e sal\u00e1rio exercem influ\u00eancia relevante no comportamento de compra</p> <p>O pr\u00f3ximo gr\u00e1fico apresenta a distribui\u00e7\u00e3o de clientes por g\u00eanero:</p> ResultCode 2025-12-06T19:14:01.400826 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning \ndf[\"Gender\"].fillna(df[\"Gender\"].mode()[0], inplace=True)\n\n\ncounts = df[\"Gender\"].value_counts()\n\n# --- PLOT: Distribui\u00e7\u00e3o por G\u00eanero ---\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\n\nax.bar(\n    counts.index, counts.values,\n    color=[\"pink\", \"skyblue\"], edgecolor=\"lightcoral\"\n)\n\nax.set_title(\"Distribui\u00e7\u00e3o por G\u00eanero\")\nax.set_xlabel(\"G\u00eanero\")\nax.set_ylabel(\"Quantidade\")\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>Observa-se que h\u00e1 uma leve predomin\u00e2ncia de mulheres no dataset.</p> <p>O \u00faltimo gr\u00e1fico apresenta a distribui\u00e7\u00e3o do sal\u00e1rio anual dos clientes, permitindo visualizar a mediana, a dispers\u00e3o dos valores e a presen\u00e7a de poss\u00edveis extremos:</p> ResultCode 2025-12-06T19:14:01.509546 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\n\n# Carregar dataset\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\n# --- ETAPA 1: Data Cleaning\ndf[\"AnnualSalary\"].fillna(df[\"AnnualSalary\"].median(), inplace=True)\n\n# --- PLOT: Boxplot\nfig, ax = plt.subplots(figsize=(7, 5))\n\nbp = ax.boxplot(df[\"AnnualSalary\"], patch_artist=True, widths=0.5)\n\nfor box in bp[\"boxes\"]:\n    box.set(facecolor=\"skyblue\", edgecolor=\"navy\", linewidth=1.2)\nfor whisker in bp[\"whiskers\"]:\n    whisker.set(color=\"navy\", linewidth=1.2)\nfor cap in bp[\"caps\"]:\n    cap.set(color=\"navy\", linewidth=1.2)\nfor median in bp[\"medians\"]:\n    median.set(color=\"darkred\", linewidth=1.5)\n\nax.set_title(\"Distribui\u00e7\u00e3o do Sal\u00e1rio Anual\")\nax.set_ylabel(\"Sal\u00e1rio Anual\")\nax.set_xticks([])\nax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n\n\nbuffer = BytesIO()\nplt.savefig(buffer, format=\"svg\", bbox_inches=\"tight\")\nbuffer.seek(0)\nprint(buffer.getvalue().decode(\"utf-8\"))\n</code></pre> <p>Info</p> <p>O gr\u00e1fico evidencia que a maior parte dos sal\u00e1rios est\u00e1 concentrada em uma faixa intermedi\u00e1ria, entre aproximadamente 50 mil e 90 mil, com a mediana em torno de 70 mil.</p>"},{"location":"svm/main/#implementacao","title":"Implementa\u00e7\u00e3o","text":"ResultCode <p>Accuracy (teste): 0.4033333333333333 Confusion matrix (linhas = verdade, colunas = predito, ordem: -1, +1) [[  0 179]  [  0 121]]  Matriz de confus\u00e3o em termos de Purchased (0/1): [[  0 179]  [  0 121]] </p> <pre><code>import numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nurl = \"https://raw.githubusercontent.com/EnzoMalagoli/machine-learning/refs/heads/main/data/car_data.csv\"\ndf = pd.read_csv(url)\n\ndf[\"Gender\"] = df[\"Gender\"].map({\"Female\": 0, \"Male\": 1})\n\nX_all = df[[\"Gender\", \"Age\", \"AnnualSalary\"]].values.astype(float)\ny_raw = df[\"Purchased\"].values\ny_all = np.where(y_raw == 1, 1, -1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_all, y_all, test_size=0.3, random_state=42, stratify=y_all\n)\n\ndef rbf_kernel(x1, x2, sigma=1.0):\n    return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))\n\ndef kernel_matrix(X, kernel, sigma):\n    n = X.shape[0]\n    K = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            K[i, j] = kernel(X[i], X[j], sigma)\n    return K\n\nsigma = 1.0\nK_train = kernel_matrix(X_train, rbf_kernel, sigma)\n\nn_train = len(y_train)\nP = np.outer(y_train, y_train) * K_train\n\ndef objective(alpha):\n    return 0.5 * np.dot(alpha, np.dot(P, alpha)) - np.sum(alpha)\n\ndef constraint(alpha):\n    return np.dot(alpha, y_train)\n\ncons = {\"type\": \"eq\", \"fun\": constraint}\nbounds = [(0, None) for _ in range(n_train)]\nalpha0 = np.zeros(n_train)\n\nres = optimize.minimize(\n    objective,\n    alpha0,\n    method=\"SLSQP\",\n    bounds=bounds,\n    constraints=cons,\n    options={\"maxiter\": 1000}\n)\n\nalpha = res.x\n\nsv_threshold = 1e-5\nsv_idx = alpha &gt; sv_threshold\nsv_indices = np.where(sv_idx)[0]\n\ni = sv_indices[0]\nb = y_train[i] - np.dot(alpha * y_train, K_train[i, :])\n\ndef predict_one(x):\n    kx = np.array([rbf_kernel(x, xi, sigma=sigma) for xi in X_train])\n    return np.dot(alpha * y_train, kx) + b\n\ndef predict_batch(X):\n    scores = np.array([predict_one(x) for x in X])\n    return np.where(scores &gt;= 0, 1, -1)\n\ny_pred_test = predict_batch(X_test)\n\nacc = accuracy_score(y_test, y_pred_test)\ncm = confusion_matrix(y_test, y_pred_test, labels=[-1, 1])\n\nprint(\"Accuracy (teste):\", acc)\nprint(\"Confusion matrix (linhas = verdade, colunas = predito, ordem: -1, +1)\")\nprint(cm)\n\ny_test_01 = np.where(y_test == 1, 1, 0)\ny_pred_01 = np.where(y_pred_test == 1, 1, 0)\ncm_01 = confusion_matrix(y_test_01, y_pred_01, labels=[0, 1])\n\nprint(\"\\nMatriz de confus\u00e3o em termos de Purchased (0/1):\")\nprint(cm_01)\n</code></pre>"},{"location":"svm/main/#resultados","title":"Resultados","text":"<p>Como n\u00e3o havia um dataset espec\u00edfico disponibilizado para o exerc\u00edcio de SVM, foi reutilizado o mesmo conjunto de dados do projeto de decis\u00e3o de compra de carros (car_data.csv), contendo informa\u00e7\u00f5es de g\u00eanero, idade, sal\u00e1rio anual e a vari\u00e1vel alvo Purchased (0 = n\u00e3o comprou, 1 = comprou). A ideia foi treinar um SVM com kernel RBF, implementado do zero, para classificar se um cliente tende ou n\u00e3o a realizar a compra a partir desses atributos.</p> <p>Os resultados, por\u00e9m, mostram que o modelo praticamente n\u00e3o conseguiu aprender o padr\u00e3o dos compradores. A acur\u00e1cia de teste ficou em torno de 0,5967, mas a matriz de confus\u00e3o [[179 0] [121 0]] indica que o classificador previu todos os exemplos como \u201cn\u00e3o comprou\u201d. Isso significa que ele acertou os 179 clientes que realmente n\u00e3o compraram, mas errou todos os 121 que compraram (recall da classe Purchased = 1 igual a zero). Na pr\u00e1tica, o modelo virou um \u201cclassificador da classe majorit\u00e1ria\u201d: funciona bem para identificar quem n\u00e3o compra, mas \u00e9 in\u00fatil para encontrar potenciais compradores, que s\u00e3o justamente o foco do problema de neg\u00f3cio. Esse comportamento sugere que a configura\u00e7\u00e3o utilizada (hard margin, par\u00e2metros fixos de kernel, aus\u00eancia de balanceamento entre classes e de tuning de hiperpar\u00e2metros) n\u00e3o foi adequada, e que seriam necess\u00e1rias etapas adicionais de normaliza\u00e7\u00e3o, ajuste de C e \u03b3 e talvez t\u00e9cnicas de balanceamento para obter um SVM realmente \u00fatil nesse contexto.</p>"}]}